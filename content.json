{"meta":{"title":"Hexo","subtitle":"","description":"This is Bear-blog!!!","author":"John Doe","url":"http://wjj-1.github.io","root":"/"},"pages":[],"posts":[{"title":"Kafka命令行操作","slug":"Kafka命令行操作","date":"2022-06-20T14:24:28.000Z","updated":"2022-06-20T14:42:29.568Z","comments":true,"path":"2022/06/20/Kafka命令行操作/","link":"","permalink":"http://wjj-1.github.io/2022/06/20/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/","excerpt":"","text":"一、kafka命令行操作1、命令行工具 kafka-topics.sh 查看帮助 2.创建topic 1./kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --create --replication-factor 3 --partitions 3 --topic test 参数说明：–replication-factor 副本数量 –partitions 分区数量 –topic topic 名称 手动指定副本的存储位置： 1bin/kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 --replica-assignment 0:1,1:2 该方式下,命令会自动判断所要创建的 topic 的分区数及副本数。 –replica-assignment 不能同时使用–partitions –replication-factor参数。指定partition的AR列表，未指定AR列表则会根据负载均衡算法将partition的replica均衡的分布在Kafka集群中。 –replica-assignment 1:3,2:1,3:2，逗号区分不同的partition，冒号区别相同partition中的replica，partition-0的AR=[1,3]，partition-1的AR=[2,1]，partition-2的AR=[3,2]。 Eg：testMcdull222AR列表计算出来时–replica-assignment 2:3,1:3,1:2 3.删除topic 1bin/kafka-topics.sh --delete --topic tpc_6 --zookeeper node1：2181 使用 kafka-topics .sh 脚本删除主题的行为本质上只是在 ZooKeeper 中的 /admin/delete_topics 路径下 建一个与待删除主题同名的节点,以标记该主题为待删除的状态。与创建主题相同的是,真正删除主题的动作也是由 Kafka 的控制器负责完成的。 4.查看topic （1）、列出当前系统中的所有 topic 1bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 –list （2）、查看 topic 详细信息 12bin/kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 --replica-assignment 0:1,1:2bin/kafka-topics.sh --describe --topic tpc_1 --zookeper node1:2181 5、增加分区数 1bin/kafka-topics.sh --alter --topic tpc_10 --partitions 4 --zookeeper node1:2181 6、动态配置 topic 参数 添加、修改配置参数（开启压缩发送传输种提高kafka消息吞吐量的有效办法(‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’)） 1bin/kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --add-config compression.type=gzip 删除配置参数 1bin/kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --delete-config compression.type","categories":[],"tags":[]},{"title":"Kafka基础配置","slug":"Kafka基础配置","date":"2022-06-18T12:10:56.000Z","updated":"2022-06-20T14:14:26.557Z","comments":true,"path":"2022/06/18/Kafka基础配置/","link":"","permalink":"http://wjj-1.github.io/2022/06/18/Kafka%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE/","excerpt":"","text":"一、Kafka环境配置1.上传安装包（移动到指定文件夹/export/servers） 1mv kafka_2.11-2.0.0.tgz /export/servers 2.解压 1tar -zxvf kafka_2.11-2.0.0.tgz 3.创建软连接 1ln -s kafka_2.11-2.0.0/ kafka 4.修改配置文件 server.properties 1vi /export/servers/kafka/config/server.properties 修改内容如下 12345678#listeners=PLAINTEXT://:9092 取消注释，内容改为：listeners=PLAINTEXT://master:9092PLAINTEXT为通信使用明文（加密ssl）log.dirs=/tmp/kafka-logs 为默认日志文件存储的位置改为log.dirs=/export/server/data/kafka-logs #数据安全性（持久化之前先放到缓存上，从缓存刷到磁盘上） 1234567interval.messages interval.ms168/24=7，1073741824/1024=1GB #数据保留策略300000ms = 300s = 5min #超过了删掉（最后修改时间还是创建时间–&gt;日志段中最晚的一条消息，维护这个最大的时间戳–&gt;用户无法 干预） #指定zookeeper集群地址 12345zookeeper.connect=localhost:2181 修改为zookeeper.connect=master:2181,slave1:2181,slave2:2181group.initial.rebalance.delay.ms=0 修改group.initial.rebalance.delay.ms=3000 5.分发kafka 1234567给 node2和 node3分发 kafkacd /export/server/scp -r /export/server/kafka_2.11-2.0.0/ node2:$PWDscp -r /export/server/kafka_2.11-2.0.0/ node3:$PWD 6.为node2，node3创建软连接 1ln -s /export/server/kafka_2.11-2.0.0/ kafka 7.配置node2 ，node3的环境变量 1234vi /etc/profile export KAFKA_HOME=/export/server/kafka export PATH=$PATH:$KAFKA_HOME/bin source /etc/profile 8.分发环境变量 12scp /etc/profile root@node2:/etc/scp /etc/profile root@node3:/etc/ 9.分别在 node2 和 node3 上修改配置文件 12345export/server/kafka/config/server.properties 中broker.id=1 broker.id=2(broker.id 不能重复)Listeners = plaintext://node2:9092 /Listeners = plaintext://node3:9092 10.启停集群(在各个节点上启动) 1kafka-server-start.sh -daemon /export/server/kafka/config/server.properties 11.停止集群 1kafka-server-stop.sh stop","categories":[],"tags":[]},{"title":"Spark local& stand-alone配置","slug":"Spark-local-stand-alone配置","date":"2022-05-28T11:32:40.000Z","updated":"2022-05-28T12:11:59.811Z","comments":true,"path":"2022/05/28/Spark-local-stand-alone配置/","link":"","permalink":"http://wjj-1.github.io/2022/05/28/Spark-local-stand-alone%E9%85%8D%E7%BD%AE/","excerpt":"","text":"Spark安装配置Spark是专为大规模数据处理而设计的快速通用的计算引擎，其提供了一个全面、统一的框架用于管理各种不同性质的数据集和数据源的大数据处理的需求，大数据开发需掌握Spark基础、SparkJob、Spark RDD、spark job部署与资源分配、Spark shuffle、Spark内存管理、Spark广播变量、Spark SQL、Spark Streaming以及Spark ML等相关知识。 一、Spark-local模式本地模式(单机) 本地模式就是以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境 Anaconda On Linux 安装 (单台服务器脚本安装) 安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 /export/server: 1234cd /export/server# 运行文件sh Anaconda3-2021.05-Linux-x86_64.sh 123456789101112过程显示：...# 出现内容选 yesPlease answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27;&gt;&gt;&gt; yes...# 出现添加路径：/export/server/anaconda3...[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3PREFIX=/export/server/anaconda3... 安装完成后, 退出终端， 重新进来: 1exit 1234结果显示：# 看到这个Base开头表明安装好了.base是默认的虚拟环境.Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1(base) [root@master ~]# 创建虚拟环境 pyspark 基于 python3.8 1conda create -n pyspark python=3.8 切换到虚拟环境内 1conda activate pyspark 123结果显示：(base) [root@master ~]# conda activate pyspark (pyspark) [root@master ~]# 在虚拟环境内安装包 （有WARNING不用管） 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple spark 安装 将文件上传到 /export/server 里面 ，解压 1234cd /export/server# 解压tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/ 建立软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark 添加环境变量 SPARK_HOME: 表示Spark安装路径在哪里PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器JAVA_HOME: 告知Spark Java在哪里HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里HADOOP_HOME: 告知Spark Hadoop安装在哪里 12345678910111213141516171819202122232425262728293031323334353637383940vim /etc/profile内容：.....注：此部分之前配置过，此部分不需要在配置#JAVA_HOMEexport JAVA_HOME=/export/server/jdk1.8.0_241 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar#HADOOP_HOMEexport HADOOP_HOME=/export/server/hadoop-3.3.0 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin#ZOOKEEPER_HOMEexport ZOOKEEPER_HOME=/export/server/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin.....# 将以下部分添加进去#SPARK_HOMEexport SPARK_HOME=/export/server/spark#HADOOP_CONF_DIRexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop#PYSPARK_PYTHONexport PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python- ```shell vim .bashrc 内容添加进去： #JAVA_HOME export JAVA_HOME=/export/server/jdk1.8.0_241 #PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python 重新加载环境变量文件 12source /etc/profilesource ~/.bashrc 进入 /export/server/anaconda3/envs/pyspark/bin/ 文件夹 1cd /export/server/anaconda3/envs/pyspark/bin/ 开启 1./pyspark 结果显示 查看WebUI界面 123浏览器访问：http://master:4040/ 退出 二、Spark-Standalone模式Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境 Anaconda On Linux 安装 (单台服务器脚本安装 注：在 slave1 和 slave2 上部署) 安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 /export/server: 1234cd /export/server# 运行文件sh Anaconda3-2021.05-Linux-x86_64.sh 123456789101112过程显示：...# 出现内容选 yesPlease answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27;&gt;&gt;&gt; yes...# 出现添加路径：/export/server/anaconda3...[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3PREFIX=/export/server/anaconda3... 安装完成后, 退出终端， 重新进来: 1exit 1234结果显示：# 看到这个Base开头表明安装好了.base是默认的虚拟环境.Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1(base) [root@master ~]# 在 master 节点上把 ./bashrc 和 profile 分发给 slave1 和 slave2 1234567#分发 .bashrc :scp ~/.bashrc root@slave1:~/scp ~/.bashrc root@slave2:~/#分发 profile :scp /etc/profile/ root@slave1:/etc/scp /etc/profile/ root@slave2:/etc/ 创建虚拟环境 pyspark 基于 python3.8 1conda create -n pyspark python=3.8 切换到虚拟环境内 1conda activate pyspark 123结果显示：(base) [root@master ~]# conda activate pyspark (pyspark) [root@master ~]# 在虚拟环境内安装包 （有WARNING不用管） 1pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple master 节点节点进入 /export/server/spark/conf 修改以下配置文件 1cd /export/server/spark/conf 将文件 workers.template 改名为 workers，并配置文件内容 123456789mv workers.template workersvim workers# localhost删除，内容追加文末：masterslave1slave2# 功能: 这个文件就是指示了 当前SparkStandAlone环境下, 有哪些worker 将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容 123456789101112131415161718192021222324252627282930313233mv spark-env.sh.template spark-env.shvim spark-env.sh文末追加内容：## 设置JAVA安装目录JAVA_HOME=/export/server/jdk## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoopYARN_CONF_DIR=/export/server/hadoop/etc/hadoop## 指定spark老大Master的IP和提交任务的通信端口# 告知Spark的master运行在哪个机器上export SPARK_MASTER_HOST=master# 告知sparkmaster的通讯端口export SPARK_MASTER_PORT=7077# 告知spark master的 webui端口SPARK_MASTER_WEBUI_PORT=8080# worker cpu可用核数SPARK_WORKER_CORES=1# worker可用内存SPARK_WORKER_MEMORY=1g# worker的工作通讯地址SPARK_WORKER_PORT=7078# worker的 webui地址SPARK_WORKER_WEBUI_PORT=8081## 设置历史服务器# 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot; 开启 hadoop 的 hdfs 和 yarn 集群 123start-dfs.shstart-yarn.sh 在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下: 123hadoop fs -mkdir /sparkloghadoop fs -chmod 777 /sparklog 将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置 1234567891011mv spark-defaults.conf.template spark-defaults.confvim spark-defaults.conf文末追加内容为：# 开启spark的日期记录功能spark.eventLog.enabled true# 设置spark日志记录的路径spark.eventLog.dir hdfs://master:8020/sparklog/ # 设置spark日志是否启动压缩spark.eventLog.compress true 配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory=INFO, console 改为 log4j.rootCategory=WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息） 123mv log4j.properties.template log4j.propertiesvim log4j.properties 12345结果显示：...18 # Set everything to be logged to the console19 log4j.rootCategory=WARN, console.... master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上 12345cd /export/server/scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ slave1:$PWDscp -r /export/server/spark-3.2.0-bin-hadoop3.2/ slave2:$PWD 在slave1 和 slave2 上做软连接 1ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark 重新加载环境变量 1source /etc/profile 进入 /export/server/spark/sbin 文件目录下 启动 start-history-server.sh 123cd /export/server/spark/sbin ./start-history-server.sh 访问 WebUI 界面 123浏览器访问：http://master:18080/ 启动Spark的Master和Worker进程 1234567891011121314151617# 启动全部master和workersbin/start-all.sh# 或者可以一个个启动:# 启动当前机器的mastersbin/start-master.sh# 启动当前机器的workersbin/start-worker.sh# 停止全部sbin/stop-all.sh# 停止当前机器的mastersbin/stop-master.sh# 停止当前机器的workersbin/stop-worker.sh 访问 WebUI界面 123浏览器访问：http://master:8080/","categories":[],"tags":[]},{"title":"Spark-HA-Yarn配置","slug":"Spark-HA-Yarn配置","date":"2022-05-27T16:00:00.000Z","updated":"2022-05-28T13:53:53.770Z","comments":true,"path":"2022/05/28/Spark-HA-Yarn配置/","link":"","permalink":"http://wjj-1.github.io/2022/05/28/Spark-HA-Yarn%E9%85%8D%E7%BD%AE/","excerpt":"","text":"Spark-Standalone-HA模式Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下 master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。 此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的 zookeeper 配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接 在 master 节点上重新进行前面配置的 zookeeper 操作 123456789101.上传apache-zookeeper-3.7.0-bin.tar.gz 到/export/server/目录下 并解压文件2.在 /export/server 目录下创建软连接3.进入 /export/server/zookeeper/conf/ 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 4.接上步给 zoo.cfg 添加内容 5.进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去6.将 master 节点中 /export/server/zookeeper-3.7.0 路径下内容推送给slave1 和 slave27.推送成功后，分别在 slave1 和 slave2 上创建软连接8.接上步推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/文件夹下的 myid 中的内容分别改为 2 和 3配置环境变量：因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此处也是创建软连接的方便之处. 进入 /export/server/spark/conf 文件夹 修改 spark-env.sh 文件内容 123cd /export/server/spark/conf vim spark-env.sh 为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都可以做 master 12345结果显示：...... 82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST=master......... 文末添加内容 1234SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;# spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现# 指定Zookeeper的连接地址# 指定在Zookeeper中注册临时节点的路径 分发 spark-env.sh 到 salve1 和 slave2 上 123scp spark-env.sh slave1:/export/server/spark/conf/scp spark-env.sh slave2:/export/server/spark/conf/ 启动之前确保 Zookeeper 和 HDFS 均已经启动 启动集群 12345# 在 master 上 启动一个master 和全部worker/export/server/spark/sbin/start-all.sh# 注意, 下面命令在 slave1 上执行 启动 slave1 上的 master 做备用 master/export/server/spark/sbin/start-master.sh 12345678910111213141516171819结果显示：(base) [root@master ~]# jps37328 DataNode41589 Master35798 QuorumPeerMain38521 ResourceManager46281 Jps38907 NodeManager41821 Worker36958 NameNode(base) [root@slave1 sbin]# jps36631 DataNode48135 Master35385 QuorumPeerMain37961 NodeManager40970 Worker48282 Jps37276 SecondaryNameNode 访问 WebUI 界面 1http://master:8081/ 1http://slave1:8082/ 此时 kill 掉 master 上的 master 假设 master 主机宕机掉 123456789101112# master主机 master 的进程号kill -9 41589结果显示：(base) [root@master ~]# jps37328 DataNode90336 Jps35798 QuorumPeerMain38521 ResourceManager38907 NodeManager41821 Worker36958 NameNode 访问 slave1 的 WebUI 1http://slave1:8082/ 进行主备切换的测试 提交一个 spark 任务到当前 活跃的 master上 : 1/export/server/spark/bin/spark-submit --master spark://master:7077 /export/server/spark/examples/src/main/python/pi.py 1000 复制标签 kill 掉 master 的 进程号 再次访问 master 的 WebUI 1http://master:8081/ 1网页访问不了！ 再次访问 slave1 的 WebUI 1http://slave1:8082/ 可以看到当前活跃的 master 提示信息 1234567(base) [root@master ~]# /export/server/spark/bin/spark-submit --master spark://master:7077 /export/server/spark/examples/src/main/python/pi.py 100022/03/29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect...22/03/29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect...Pi is roughly 3.140960(base) [root@master ~]# ​ 同样可以输出结果 当新的 master 接收集群后, 程序继续运行, 正常得到结果. 结论 HA模式下, 主备切换 不会影响到正在运行的程序. 最大的影响是 会让它中断大约30秒左右. Spark On YARN模式在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无需部署Spark集群, 只要找一台服务器, 充当Spark的客户端 保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了） 1234567spark-env.sh 文件部分显示：.... 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop 79 YARN_CONF_DIR=/export/server/hadoop/etc/hadoop.... 链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式） 123456789bin/pyspark --master yarn --deploy-mode client|cluster# --deploy-mode 选项是指定部署模式, 默认是 客户端模式# client就是客户端模式# cluster就是集群模式# --deploy-mode 仅可以用在YARN模式下1.2.- ```shell3.bin/spark-shell --master yarn --deploy-mode client|cluster 123456789101112bin/spark-submit --master yarn --deploy-mode client|cluster /xxx/xxx/xxx.py 参数1.2.- spark-submit 和 spark-shell 和 pyspark的相关参数3.- bin/pyspark: pyspark解释器spark环境- bin/spark-shell: scala解释器spark环境- bin/spark-submit: 提交jar包或Python文件执行的工具- bin/spark-sql: sparksql客户端工具 12这4个客户端工具的参数基本通用.以spark-submit 为例:bin/spark-submit --master spark://master:7077 xxx.py` 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576```shellUsage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]Usage: spark-submit --kill [submission ID] --master [spark://...]Usage: spark-submit --status [submission ID] --master [spark://...]Usage: spark-submit run-example [options] example-class [example args]Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, k8s://https://host:port, or local (Default: local[*]). --deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client --class CLASS_NAME 运行java或者scala class(for Java / Scala apps). --name NAME 程序的名字 --jars JARS Comma-separated list of jars to include on the driver and executor classpaths. --packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by --repositories. The format for the coordinates should be groupId:artifactId:version. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts. --repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages. --py-files PY_FILES 指定Python程序依赖的其它python文件 --files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName). --archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor. --conf, -c PROP=VALUE 手动指定配置 --properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf. --driver-memory MEM Driver的可用内存(Default: 1024M). --driver-java-options Driver的一些Java选项 --driver-library-path Extra library path entries to pass to the driver. --driver-class-path Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath. --executor-memory MEM Executor的内存 (Default: 1G). --proxy-user NAME User to impersonate when submitting the application. This argument does not work with --principal / --keytab. --help, -h 显示帮助文件 --verbose, -v Print additional debug output. --version, 打印版本 Cluster deploy mode only(集群模式专属): --driver-cores NUM Driver可用的的CPU核数(Default: 1). Spark standalone or Mesos with cluster deploy mode only: --supervise 如果给定, 可以尝试重启Driver Spark standalone, Mesos or K8s with cluster deploy mode only: --kill SUBMISSION_ID 指定程序ID kill --status SUBMISSION_ID 指定程序ID 查看运行状态 Spark standalone, Mesos and Kubernetes only: --total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用 Spark standalone, YARN and Kubernetes only: --executor-cores NUM 单个Executor能使用多少CPU核心 Spark on YARN and Kubernetes only(YARN模式下): --num-executors NUM Executor应该开启几个 --principal PRINCIPAL Principal to be used to login to KDC. --keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only: --queue QUEUE_NAME 指定运行的YARN队列(Default: &quot;default&quot;). 启动 YARN 的历史服务器 123cd /export/server/hadoop-3.3.0/sbin./mr-jobhistory-daemon.sh start historyserver 访问WebUI界面 1http://master:19888/ client 模式测试 123SPARK_HOME=/export/server/spark $&#123;SPARK_HOME&#125;/bin/spark-submit --master yarn --deploy-mode client --driver-memory 512m --executor-memory 512m --num-executors 1 --total-executor-cores 2 $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3 cluster 模式测试 123SPARK_HOME=/export/server/spark $&#123;SPARK_HOME&#125;/bin/spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m --num-executors 1 --total-executor-cores 2 --conf &quot;spark.pyspark.driver.python=/root/anaconda3/bin/python3&quot; --conf &quot;spark.pyspark.python=/root/anaconda3/bin/python3&quot; $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3","categories":[],"tags":[]},{"title":"Spark基础配置","slug":"Spark基础配置","date":"2022-05-22T08:11:29.000Z","updated":"2022-05-28T12:30:20.249Z","comments":true,"path":"2022/05/22/Spark基础配置/","link":"","permalink":"http://wjj-1.github.io/2022/05/22/Spark%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE/","excerpt":"","text":"一．基础环境 导入三台虚拟机，三台虚拟机信息汇总。 主机名 Node1.itcast.cn Node2.itcast.cn Node3.itcast.cn IP 192.168.88.151 192.168.88.152 192.168.88.153 用户名、密码 root/123456 root/123456 root/123456 集群角色规划 服务器 运行角色 Node1.itcast.cn Namenode(主角色)，Datanode(从角色)，Resourcemanager(主角色)Nodemanager(从角色)，master，follower，worker，QuorumPeerMain,sparksubmit Node2.itcast.cn Secondarynamenode(主角色辅助角色)，datanode(从角色)Nodemanager(从角色)，worker，leader Node3.itcast.cn Datanode(从角色)，nodemanager(从角色)，worker，follower （1）主机hosts映射 1cat hosts （2）关闭防火墙 1systemctl status firewalld.service （3）同步时间 1ntpdate ntp5.aliyun.com （4）配置ssh 免密登录node1、2、3 123ssh node1ssh node2ssh node3 二、安装并配置JDK （1）编译环境软件安装目录 1mkdir -p /export/servers （2）上传 jdk-8u171-linux-x64.tar.gz到/export/softwares/目录下 安装JDK 解压文件至/export/servers/ 1tar -zxvf /export/softwares/jdk-8u171-linux-x64.tar.gz -C /export/esrvers JDK安装目录重命名为jdk 1mv /export/servers/jdk1.8.0_171/ jdk （3）配置环境变量 重新加载环境变量文件 1source /etc/profile （4）JDK环境验证 1java -version （5）分发JDK相关文件到node2、3 1.jdk相关文件分发 123scp -r /export/servers/jdk1.8.0_171/ root@slave1:/export/servers/ scp -r /export/servers/jdk1.8.0_171/ root@slave2:/export/servers/ 2.分发系统环境变量文件至slave1,slave2 123scp -r /etc/profile root@slave1:/etc/profile scp -r /etc/profile root@slave2:/etc/profile 3.分别在slave1、slave2上执行source /etc/profile使环境变量生效 三、Hadoop集群的安装和配置 1.把 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 /export/server 并解压文件 1tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 2.修改配置文件(进入路径 /export/server/hadoop-3.3.0/etc/hadoop) 1cd /export/server/hadoop-3.3.0/etc/hadoop 3.hadoop-env.sh 12345678#文件最后添加export JAVA_HOME=/export/server/jdk1.8.0_241export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root 4.core-site.xml 12345678910111213141516171819202122232425262728293031323334&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:8020&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置Hadoop本地保存数据路径 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置HDFS web UI用户身份 --&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;!-- 整合hive 用户代理设置 --&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;!-- 文件系统垃圾桶保存时间 --&gt;&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt;&lt;/property&gt; 4.hdfs-site.xml 12345&lt;!-- 设置SNN进程运行机器位置信息 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;slave1:9868&lt;/value&gt;&lt;/property&gt; 5.mapred-site.xml 1234567891011121314151617181920212223242526272829303132&lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;!-- MR程序历史服务地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt;&lt;/property&gt; &lt;!-- MR程序历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;&lt;/property&gt; 6.yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施物理内存限制 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启日志聚集 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置yarn历史服务器地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://master:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史日志保存的时间 7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 7.workers 123masterslave1slave2 8.分发同步hadoop安装包 1234cd /export/serverscp -r hadoop-3.3.0 root@slave1:$PWDscp -r hadoop-3.3.0 root@slave2:$PWD 9.将hadoop添加到环境变量 1234vim /etc/profileexport HADOOP_HOME=/export/server/hadoop-3.3.0export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 10.重新加载环境变量文件 1source /etc/profile Hadoop集群启动 格式化namenode（只有首次启动需要格式化） 1hdfs namenode -format （1）脚本一键启动 12345678910111213[root@master ~]# start-dfs.sh Starting namenodes on [master]上一次登录：五 3月 11 21:27:24 CST 2022pts/0 上Starting datanodes上一次登录：五 3月 11 21:27:32 CST 2022pts/0 上Starting secondary namenodes [slave1]上一次登录：五 3月 11 21:27:35 CST 2022pts/0 上[root@master ~]# start-yarn.sh Starting resourcemanager上一次登录：五 3月 11 21:27:41 CST 2022pts/0 上Starting nodemanagers上一次登录：五 3月 11 21:27:51 CST 2022pts/0 上 （2）启动后，输入jps查看 1234567891011121314151617[root@master ~]# jps127729 NameNode127937 DataNode14105 Jps128812 NodeManager128591 ResourceManager[root@slave1 hadoop]# jps121889 NodeManager121559 SecondaryNameNode7014 Jps121369 DataNode[root@slave2 hadoop]# jps6673 Jps121543 NodeManager121098 DataNode WEB页面 HDFS集群 1http://master:9870/ YARN集群 1http://master:8088/","categories":[],"tags":[]}],"categories":[],"tags":[]}